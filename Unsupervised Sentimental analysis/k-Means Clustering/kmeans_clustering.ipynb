{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = Word2Vec.load(\"word2vec.model\").wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thei', 0.7631075382232666),\n",
       " ('gammon', 0.729962944984436),\n",
       " ('decent', 0.7257193922996521),\n",
       " ('mostly', 0.7242159843444824),\n",
       " ('failing', 0.721838653087616),\n",
       " ('th', 0.7167788743972778),\n",
       " ('nah', 0.7133491039276123),\n",
       " ('peo', 0.7103697061538696),\n",
       " ('benefit', 0.7052839994430542),\n",
       " ('blackli', 0.7034202814102173),\n",
       " ('abou', 0.7015049457550049)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[0], topn=11, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRIYANKA\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "words = pd.DataFrame(word_vectors.vocab.keys())\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>alllivesmatter</td>\n",
       "      <td>[0.03352602, -0.043020952, -0.00232427, 0.0180...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.054813</td>\n",
       "      <td>1.054813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>look_like</td>\n",
       "      <td>[-0.034277424, 0.018904887, 0.07060962, 0.0342...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.130751</td>\n",
       "      <td>1.130751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>attendant</td>\n",
       "      <td>[-0.054576255, 0.07015803, -0.03585131, 0.0331...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.140103</td>\n",
       "      <td>-1.140103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>[0.018800395, -0.10510669, 0.10309875, 0.01638...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.096741</td>\n",
       "      <td>1.096741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>graduate</td>\n",
       "      <td>[0.05906425, 0.023874968, -0.005894852, 0.0327...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.079865</td>\n",
       "      <td>1.079865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>but</td>\n",
       "      <td>[0.04050105, -0.044139367, 0.015760371, -0.001...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.271640</td>\n",
       "      <td>1.271640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>deranged</td>\n",
       "      <td>[-0.027293492, -0.017787265, 0.09804127, 0.027...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.170161</td>\n",
       "      <td>1.170161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ignorant</td>\n",
       "      <td>[-0.024534395, -0.008489642, 0.00914253, 0.104...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.112257</td>\n",
       "      <td>1.112257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>incompetent</td>\n",
       "      <td>[0.001806536, -0.037414446, 0.04319391, 0.0638...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.098314</td>\n",
       "      <td>1.098314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>r</td>\n",
       "      <td>[0.017156366, 0.020579617, -0.026792856, 0.110...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.028735</td>\n",
       "      <td>1.028735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words                                            vectors  \\\n",
       "0   alllivesmatter  [0.03352602, -0.043020952, -0.00232427, 0.0180...   \n",
       "1        look_like  [-0.034277424, 0.018904887, 0.07060962, 0.0342...   \n",
       "2        attendant  [-0.054576255, 0.07015803, -0.03585131, 0.0331...   \n",
       "3  realdonaldtrump  [0.018800395, -0.10510669, 0.10309875, 0.01638...   \n",
       "4         graduate  [0.05906425, 0.023874968, -0.005894852, 0.0327...   \n",
       "5              but  [0.04050105, -0.044139367, 0.015760371, -0.001...   \n",
       "6         deranged  [-0.027293492, -0.017787265, 0.09804127, 0.027...   \n",
       "7         ignorant  [-0.024534395, -0.008489642, 0.00914253, 0.104...   \n",
       "8      incompetent  [0.001806536, -0.037414446, 0.04319391, 0.0638...   \n",
       "9                r  [0.017156366, 0.020579617, -0.026792856, 0.110...   \n",
       "\n",
       "   cluster  cluster_value  closeness_score  sentiment_coeff  \n",
       "0        0              1         1.054813         1.054813  \n",
       "1        0              1         1.130751         1.130751  \n",
       "2        1             -1         1.140103        -1.140103  \n",
       "3        0              1         1.096741         1.096741  \n",
       "4        0              1         1.079865         1.079865  \n",
       "5        0              1         1.271640         1.271640  \n",
       "6        0              1         1.170161         1.170161  \n",
       "7        0              1         1.112257         1.112257  \n",
       "8        0              1         1.098314         1.098314  \n",
       "9        0              1         1.028735         1.028735  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNING CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = pd.read_csv('cleaned_datasets.csv')\n",
    "#tweet_file=pd.read_csv('USelectionw2v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = pd.read_csv('sentiment_dictionary.csv')\n",
    "sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_weighting = final_file.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf.fit(file_weighting.full_text)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(file_weighting.full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    '''\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "    \n",
    "    inspired  by function from this wonderful article: \n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    \n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "\n",
    "    '''\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    '''\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x.full_text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)#this step takes around 3-4 minutes minutes to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    '''\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    '''\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_closeness_scores = file_weighting.full_text.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.title, file_weighting.rate]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence', 'sentiment']\n",
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.full_text]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence']\n",
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n",
    "replacement_df['Tweet']=final_file.Tweet\n",
    "replacement_df['Hashtags']=final_file.hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df['prediction'] = replacement_df['prediction'].apply(lambda x: 1 if x==0 else -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_coeff</th>\n",
       "      <th>tfidf_scores</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment_rate</th>\n",
       "      <th>prediction</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1.0548129096757086]</td>\n",
       "      <td>[10.77741399528076, 3.840099914057079]</td>\n",
       "      <td>traceybra alllivesmatter</td>\n",
       "      <td>4.050587</td>\n",
       "      <td>-1</td>\n",
       "      <td>@traceybra @TheLondonHughes #AllLivesMatter</td>\n",
       "      <td>alllivesmatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[1.1307508195068914, 0, -1.1401028216133509]</td>\n",
       "      <td>[6.010975661696547, 9.273336598504486, 9.07266...</td>\n",
       "      <td>look_like easyjet_flight attendant</td>\n",
       "      <td>-3.546856</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @beardedfinance: You look like an EasyJet f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1.0548129096757086, 1.0548129096757086, 1....</td>\n",
       "      <td>[10.77741399528076, 26.880699398399553, 26.880...</td>\n",
       "      <td>lieve alllivesmatter alllivesmatter alllivesma...</td>\n",
       "      <td>198.478761</td>\n",
       "      <td>-1</td>\n",
       "      <td>RT @EWindt: Lieve @NUnl  #AllLivesMatter #AllL...</td>\n",
       "      <td>alllivesmatter,alllivesmatter,alllivesmatter,a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[1.0967411557102231, 0, 1.0798650595378538, 1....</td>\n",
       "      <td>[3.6790383566899734, 9.16797608284666, 8.13835...</td>\n",
       "      <td>realdonaldtrump congrats graduate but embarras...</td>\n",
       "      <td>52.686019</td>\n",
       "      <td>-1</td>\n",
       "      <td>@realDonaldTrump Congrats to the graduates. Bu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[-1.1048555385771512, 1.1587823894226137, 1.15...</td>\n",
       "      <td>[8.580189417944542, 5.46174799039811, 5.309353...</td>\n",
       "      <td>secretsbedard_thezogbypoll country want law_or...</td>\n",
       "      <td>34.289739</td>\n",
       "      <td>-1</td>\n",
       "      <td>RT @deplorablelori: @SecretsBedard @realDonald...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>[1.0967411557102231, 1.1338393406851608, 1.122...</td>\n",
       "      <td>[3.6790383566899734, 7.558538170412559, 8.5261...</td>\n",
       "      <td>realdonaldtrump black_community need_stand pre...</td>\n",
       "      <td>47.428534</td>\n",
       "      <td>-1</td>\n",
       "      <td>RT @LittleMike1977: @realDonaldTrump “The Blac...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>[1.1209469991455219, 0, 0, 1.004533223931562, ...</td>\n",
       "      <td>[12.01345874163019, 9.524651026785392, 19.0493...</td>\n",
       "      <td>cop many_victim buffalo v orlando 13 people ki...</td>\n",
       "      <td>28.343042</td>\n",
       "      <td>-1</td>\n",
       "      <td>Cops have too many victims. Buffalo vs. Orland...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>[0, 0, 1.1384337409032284, 1.2052243082191785,...</td>\n",
       "      <td>[10.084266814720815, 9.16797608284666, 6.55790...</td>\n",
       "      <td>hackneyabbott deluded idiot ignoring video blm...</td>\n",
       "      <td>62.480874</td>\n",
       "      <td>-1</td>\n",
       "      <td>@HackneyAbbott You deluded idiot. You're ignor...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>[0, 1.1192989212489453, 0, 1.2009581038748272,...</td>\n",
       "      <td>[10.371948887172596, 8.835680253216765, 9.1679...</td>\n",
       "      <td>berniebros amp alt left fucking w/ amp hard wo...</td>\n",
       "      <td>67.297141</td>\n",
       "      <td>-1</td>\n",
       "      <td>Why is @AOC  #BernieBros &amp;amp; the alt-left fu...</td>\n",
       "      <td>berniebros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>[0.997705655358815, 1.0548129096757086, 1.0461...</td>\n",
       "      <td>[3.479307246000507, 3.840099914057079, 7.75698...</td>\n",
       "      <td>blacklivesmatter alllivesmatter backtheblue</td>\n",
       "      <td>15.636953</td>\n",
       "      <td>-1</td>\n",
       "      <td>RT @GiveLatte: #BlackLivesMatter #AllLivesMatt...</td>\n",
       "      <td>blacklivesmatter,alllivesmatter,backtheblue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sentiment_coeff  \\\n",
       "0                             [0, 1.0548129096757086]   \n",
       "1        [1.1307508195068914, 0, -1.1401028216133509]   \n",
       "2   [0, 1.0548129096757086, 1.0548129096757086, 1....   \n",
       "3   [1.0967411557102231, 0, 1.0798650595378538, 1....   \n",
       "4   [-1.1048555385771512, 1.1587823894226137, 1.15...   \n",
       "..                                                ...   \n",
       "95  [1.0967411557102231, 1.1338393406851608, 1.122...   \n",
       "96  [1.1209469991455219, 0, 0, 1.004533223931562, ...   \n",
       "97  [0, 0, 1.1384337409032284, 1.2052243082191785,...   \n",
       "98  [0, 1.1192989212489453, 0, 1.2009581038748272,...   \n",
       "99  [0.997705655358815, 1.0548129096757086, 1.0461...   \n",
       "\n",
       "                                         tfidf_scores  \\\n",
       "0              [10.77741399528076, 3.840099914057079]   \n",
       "1   [6.010975661696547, 9.273336598504486, 9.07266...   \n",
       "2   [10.77741399528076, 26.880699398399553, 26.880...   \n",
       "3   [3.6790383566899734, 9.16797608284666, 8.13835...   \n",
       "4   [8.580189417944542, 5.46174799039811, 5.309353...   \n",
       "..                                                ...   \n",
       "95  [3.6790383566899734, 7.558538170412559, 8.5261...   \n",
       "96  [12.01345874163019, 9.524651026785392, 19.0493...   \n",
       "97  [10.084266814720815, 9.16797608284666, 6.55790...   \n",
       "98  [10.371948887172596, 8.835680253216765, 9.1679...   \n",
       "99  [3.479307246000507, 3.840099914057079, 7.75698...   \n",
       "\n",
       "                                             sentence  sentiment_rate  \\\n",
       "0                            traceybra alllivesmatter        4.050587   \n",
       "1                  look_like easyjet_flight attendant       -3.546856   \n",
       "2   lieve alllivesmatter alllivesmatter alllivesma...      198.478761   \n",
       "3   realdonaldtrump congrats graduate but embarras...       52.686019   \n",
       "4   secretsbedard_thezogbypoll country want law_or...       34.289739   \n",
       "..                                                ...             ...   \n",
       "95  realdonaldtrump black_community need_stand pre...       47.428534   \n",
       "96  cop many_victim buffalo v orlando 13 people ki...       28.343042   \n",
       "97  hackneyabbott deluded idiot ignoring video blm...       62.480874   \n",
       "98  berniebros amp alt left fucking w/ amp hard wo...       67.297141   \n",
       "99        blacklivesmatter alllivesmatter backtheblue       15.636953   \n",
       "\n",
       "    prediction                                              Tweet  \\\n",
       "0           -1        @traceybra @TheLondonHughes #AllLivesMatter   \n",
       "1            1  RT @beardedfinance: You look like an EasyJet f...   \n",
       "2           -1  RT @EWindt: Lieve @NUnl  #AllLivesMatter #AllL...   \n",
       "3           -1  @realDonaldTrump Congrats to the graduates. Bu...   \n",
       "4           -1  RT @deplorablelori: @SecretsBedard @realDonald...   \n",
       "..         ...                                                ...   \n",
       "95          -1  RT @LittleMike1977: @realDonaldTrump “The Blac...   \n",
       "96          -1  Cops have too many victims. Buffalo vs. Orland...   \n",
       "97          -1  @HackneyAbbott You deluded idiot. You're ignor...   \n",
       "98          -1  Why is @AOC  #BernieBros &amp; the alt-left fu...   \n",
       "99          -1  RT @GiveLatte: #BlackLivesMatter #AllLivesMatt...   \n",
       "\n",
       "                                             Hashtags  \n",
       "0                                      alllivesmatter  \n",
       "1                                                 NaN  \n",
       "2   alllivesmatter,alllivesmatter,alllivesmatter,a...  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "..                                                ...  \n",
       "95                                                NaN  \n",
       "96                                                NaN  \n",
       "97                                                NaN  \n",
       "98                                         berniebros  \n",
       "99        blacklivesmatter,alllivesmatter,backtheblue  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df[['Tweet','Hashtags','prediction']].to_csv('final_prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
